{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d189ab-5e1e-4c8d-b9d3-1f97d062ffae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Crash Course on Surviving the Math\n",
    "Week One - July 25, 2022\n",
    "\n",
    "## Chapter 2 - First, a Minimal Intro to Python\n",
    "For a deeper dive read Chapter 2 and as a quick reference, see [the Python 3 Cheat Sheet](https://www.pythoncheatsheet.org/cheatsheet/basics).\n",
    "\n",
    "Note that a lot of the code here is *not* following best practices for coding consistancy and organization, but rather is attempting to show a variety of methods to accomplish similar results. If your team has a coding standard, follow it please (seriously, it is important for sane software engineering!)\n",
    "\n",
    "### Setting up the Environment\n",
    "\n",
    "If you are on your own system (instead of this ephemerial notebook) instead of the below 'conda' commands, run the following on your system command prompt (you only have to do it once for your own system, not every time):\n",
    "\n",
    "<code>pip install numpy, matplotlib, tqdm, scikit-learn</code>\n",
    "\n",
    "If would like the Jupyter environment on your own system, you can install Jupyter Notebooks via:\n",
    "\n",
    "<code> pip install jupyter </code>\n",
    "\n",
    "Then you can either work in VS Code, Notepad, or you can launch a localhost Jupyter notebook with the command:\n",
    "\n",
    "<code>jupyter notebook</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36823a-49a2-4e6c-b27b-5bb17ad8e1fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is how you import additional functionality via libraries:\n",
    "import math\n",
    "import sys\n",
    "# This is how you install new packages in Jupyter Notebooks (when you don't have system access)\n",
    "!conda install --yes --prefix {sys.prefix} numpy\n",
    "!conda install --yes --prefix {sys.prefix} matplotlib\n",
    "!conda install --yes --prefix {sys.prefix} tqdm\n",
    "!conda install --yes --prefix {sys.prefix} scikit-learn\n",
    "\n",
    "# .. and then include them\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007b6b3-c582-40d8-a9f5-42f9e018dda8",
   "metadata": {},
   "source": [
    "### Using the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01370e-5035-4309-9599-5793dbb2072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python Commentm which starts with a '#' sign\n",
    "# Below is Python Code\n",
    "\n",
    "\n",
    "# This is how you declare and assign a scalar variable. Variables contain values that can also change.\n",
    "myValue = 234.234\n",
    "\n",
    "# This is how you declare and assign an array variable of multiple values (Numpy will use proper 'vectors')\n",
    "myVector = [1, 1, 2, 3, 5, 8, 13]\n",
    "\n",
    "# This is how you print out the value of a variable\n",
    "print(\"My scalar value is %s\" % myValue)\n",
    "print(\"My vector has the list of values: %s\" % myVector)\n",
    "print(\"My vector's first element (zero-th index): %s\" % myVector[0])\n",
    "\n",
    "# You can do math:\n",
    "print(\"The product of Value and 2 = %s\" % (myValue * 2))\n",
    "print(\"Value divided by 2 = %s\" % (myValue / 2))\n",
    "print(\"Square root of Value = %s\" % (math.sqrt(myValue)))\n",
    "print(\"Square root of Value = %s\" % (myValue ** 0.5))\n",
    "\n",
    "# You can manipulate the array:\n",
    "myVector.reverse()\n",
    "print(\"Reversed vector has the list of values: %s\" % myVector)\n",
    "print(\"Reversed vector's first element (zero-th index): %s\" % myVector[0])\n",
    "\n",
    "# This is how you define a function... btw, this particular function will be important for perceptrons and other normalization of values to (0.0, 1.0) \n",
    "def sigmoidal(x):\n",
    "    return (1.0 / (1.0 + np.e ** (0-x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b760a0e-8e84-4c6b-892a-d6c0f7f56833",
   "metadata": {},
   "source": [
    "## Chapter 3 - Visualizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f67758a-9d38-4f06-968b-2f9027db6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-2 * np.pi, 2 * np.pi, 100)\n",
    "y1 = np.sin(x)\n",
    "y2 = np.cos(x)\n",
    "y3 = np.tanh(x)  # <--- Remember this \"Sigmoidal\" function for later (Sigmoid from Sigma from the greek letter, which in English starts with an 'S' and this curve is 'S' shaped)\n",
    "y4 = [sigmoidal(i) for i in x] # <--- This is the other Sigmoid function - remember it too\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y1, color='r', label='sine')\n",
    "ax.plot(x, y2, color='g', label='cos')\n",
    "ax.plot(x, y3, color='b', label='tanh')\n",
    "ax.plot(x, y4, color='y', label='sigmoid')\n",
    "plt.gcf().set_size_inches(15,10)\n",
    "plt.xlabel(\"This is the x-axis for independant variables\")\n",
    "plt.ylabel(\"This is the y-axis for dependant variables\")\n",
    "plt.title(\"My chart is titled 'Trig Graphs and another Sigmoid'\")\n",
    "plt.grid(True, which='both')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a513f6a0-d39d-4bc9-9aab-eeef276c9a97",
   "metadata": {},
   "source": [
    "## Chapter 4 - Linear Algebra\n",
    "### The Second Most Important Equation in Machine Learning\n",
    "There are [may different 'distance' approaches](https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa), but the most common is one we are familiar with by other names and simpler forms:\n",
    "\n",
    "\n",
    "![Euclidian Distance](https://miro.medium.com/max/844/0*wv6oFAVd0_PQ50mX)\n",
    "\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2169f78a-2613-413c-b82f-ff5f0646b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidianDistance(a, b):\n",
    "    result = 0\n",
    "    if len(a) == len(b):\n",
    "        summation = 0\n",
    "        for i in range(len(a)):\n",
    "            summation = (a[i] - b[i]) ** 2\n",
    "        result = summation ** 0.5\n",
    "    else:\n",
    "        print(\"WARNING: array sizes must be the same!\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717430a-a222-4ff7-8098-9617e705f639",
   "metadata": {},
   "source": [
    "The magnitude of the vector is effectively the distance from an 'origin' (all zero values) vector.  The L<sub>2</sub>-Norm, sometimes noted |X|<sub>2</sub>, is the Euclidian magnitude or Euclidian distance to the origin. Likewise the L<sub>1</sub>-Norm, sometimes noted |X|<sub>1</sub>, is the Manhattan distance to the origin.\n",
    "\n",
    "L<sub>2</sub>-Norm = ![L2-Norm](https://wikimedia.org/api/rest_v1/media/math/render/svg/50aa64b52c9ebd19d47552eae57ec4a05cf43e67)         and likewise the L<sub>p</sub>-Norm =  ![LP-Norm](https://wikimedia.org/api/rest_v1/media/math/render/svg/53a5615d02f7a03013e22bd4adf055cdbe4a303c)\n",
    "\n",
    "Related is the Root-mean-squared (RMS) Error, which adds a normalizing term dividing the summation by the vector size before taking the square root. \n",
    "\n",
    "\n",
    "![RMSE](https://www.gstatic.com/education/formulas2/443397389/en/root_mean_square.svg)\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4040be-8aef-4c3e-b1b7-d6d7858c95a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(hypothesis, truth):\n",
    "    result = 0\n",
    "    if len(hypothesis) == len(truth):\n",
    "        summation = 0\n",
    "        for i in range(len(hypothesis)):\n",
    "            summation = (hypothesis[i] - truth[i]) ** 2\n",
    "        result = (summation/len(hypothesis)) ** 0.5        # <--- differs from Euclidian Distance here\n",
    "    else:\n",
    "        print(\"WARNING: array sizes must be the same!\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b92b08-5092-4073-a06b-f91ed000021f",
   "metadata": {},
   "source": [
    "\n",
    "... but in the end, these are all forms of the [Minkowski Distance](https://en.wikipedia.org/wiki/Minkowski_distance): \n",
    "\n",
    "![Minkowski Distance](https://miro.medium.com/max/902/0*UbbyH2MUPb5ZBa64)\n",
    "\n",
    "In code form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d45682-2263-4833-9583-407f28ce5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski(a, b, p):\n",
    "    result = 0\n",
    "    if len(a) == len(b):\n",
    "        summation = 0\n",
    "        maxDiff = 0\n",
    "        for i in range(len(a)):\n",
    "            if p < 500:\n",
    "                summation += abs(a[i] - b[i]) ** p\n",
    "            maxDiff = max(maxDiff, abs(a[i] - b[i]))\n",
    "        if p > 500:\n",
    "            result = maxDiff\n",
    "        else:\n",
    "            result = summation ** (1.0/p)\n",
    "    else:\n",
    "        print(\"WARNING: array sizes must be the same!\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a99423-e402-4f6f-a51c-9f8f66863728",
   "metadata": {},
   "source": [
    "\n",
    "The value of 'p' transforms the equation from Manhattan distance (p=0, a.k.a. L<sub>1</sub>-Norm) to Euclidian distance (p=1, a.k.a. L<sub>2</sub>-Norm), or [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance) as p approaches infinity (which is effectively the max distance of any present index). For fractional values of 'p' where 0 < p < 1, the 'Agrawal' distance is extremely useful to mitigate the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). \n",
    "\n",
    "![Variations of P](https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/2D_unit_balls.svg/967px-2D_unit_balls.svg.png)\n",
    "\n",
    "... and because I can't miss an XKCD opportunity:\n",
    "\n",
    "![XKCD2426](https://imgs.xkcd.com/comics/minkowski_space.png)\n",
    "<center>XKCD 2646 We were able to follow the ship into Minkowski space, but now they've jumped to Hilbert space and they could honestly be anywhere.</center>\n",
    "\n",
    "### In Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ee313-e74c-4434-88ce-e3361e40240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N = 4\n",
    "vec0 = [ 0 for x in range(N)]\n",
    "vec1 = [random.gauss(5, 2) for x in range(N)] \n",
    "vec2 = [random.gauss(10, 2) for x in range(N)] \n",
    "vec1.sort()\n",
    "vec2.sort()\n",
    "\n",
    "print(\"Vector 0: %s\" % vec0)\n",
    "print(\"Vector 1: %s\" % vec1)\n",
    "print(\"Vector 2: %s\" % vec2)\n",
    "\n",
    "print(\"L1-Norm of Vector 1:{:>13,.2f}\".format(minkowski(vec1, vec0, 1)))\n",
    "print(\"L1-Norm of Vector 2:{:>13,.2f}\".format(minkowski(vec2, vec0, 1)))\n",
    "print(\"L2-Norm of Vector 1:{:>13,.2f}\".format(minkowski(vec1, vec0, 2)))\n",
    "print(\"L2-Norm of Vector 2:{:>13,.2f}\".format(minkowski(vec2, vec0, 2)))\n",
    "\n",
    "print(\"Agrawal   Distance: {:>13,.2f}\".format(minkowski(vec1, vec2,    0.1)))\n",
    "print(\"Manhattan Distance: {:>13,.2f}\".format(minkowski(vec1, vec2,    1)))\n",
    "print(\"Euclidian Distance: {:>13,.2f}\".format(minkowski(vec1, vec2,    2)))\n",
    "print(\"Chebyshev Distance: {:>13,.2f}\".format(minkowski(vec1, vec2, 3000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fd8edc-83e6-45bb-8799-1f39e94bbdd0",
   "metadata": {},
   "source": [
    "### Matrix\n",
    "\n",
    "A Matrix is essentially an array of arrays. Stack them further and it is called a tensor (hence TensorFlow).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1a9d0-218d-474d-ac95-496c59bc811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = [[1, 2, 3],[4, 5, 6]]\n",
    "\n",
    "print(\"Matrix m = %s\" % m)\n",
    "print(\"Element 1,2 is %s\" % m[1][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa7df3-5e2a-423c-a25d-608aa5f3204e",
   "metadata": {},
   "source": [
    "## Chapter 5 - Statistics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c9500-6e4f-4789-9a7d-3b3833928445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# The first statistical moment\n",
    "def mean(xs: List[float]) -> float:\n",
    "    return sum(xs) / len(xs)\n",
    "\n",
    "# normalize the set to mean = 0\n",
    "def de_mean(xs: List[float]) -> float:\n",
    "    x_bar = mean(xs)\n",
    "    return [x - x_bar for x in xs]\n",
    "\n",
    "# The second statistical moment\n",
    "# Variance is the average distance to the average\n",
    "def variance(xs: List[float]) -> float:\n",
    "    dm = de_means(xs)\n",
    "    return np.dot(dm,dm) / (len(xs) - 1)\n",
    "\n",
    "\n",
    "def covariance(xs: List[float], ys: List[float]) -> float:\n",
    "    return np.dot(de_means(xs), de_means(ys)) / (len(xs) - 1)\n",
    "\n",
    "def corr(xs: List[float], ys: List[float]) -> float:\n",
    "    stddev_x = variance(xs) ** 0.5 # variance is 'sigma-squared', so square-root of variance = standard deviation\n",
    "    stddev_y = variance(ys) ** 0.5\n",
    "    if stddev_x > 0 and stddev_y > 0:\n",
    "        return covariance(xs, ys) / stddev_x / stddev_y\n",
    "    else:\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a111c5-8de3-4dea-aa2d-462c5f0a6210",
   "metadata": {},
   "source": [
    "## Chapter 6 - Probability\n",
    "\n",
    "### Probability Distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f3141-fc80-460d-b42d-5ffc973a8a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "\n",
    "# Uniform random between 0.0-1.0\n",
    "randX = random.random()\n",
    "\n",
    "# If N is low, the plots do not necessarily take shape. The number must be large enough to get \"sufficient statistics\"\n",
    "N = 20\n",
    "mean = 5\n",
    "spread = 5\n",
    "\n",
    "uniformSet  = []\n",
    "gaussianSet = []\n",
    "paretoSet   = []\n",
    "weibullSet  = []\n",
    "\n",
    "for i in range(1,N):\n",
    "    uniformSet.append(round(random.uniform(mean-spread,mean+spread),3))\n",
    "    gaussianSet.append(round(random.gauss(mean,spread),3))\n",
    "    paretoSet.append(round(random.paretovariate(mean),3))\n",
    "    weibullSet.append(round(random.weibullvariate(mean,spread),3))\n",
    "\n",
    "uniformSet.sort()\n",
    "gaussianSet.sort()\n",
    "paretoSet.sort()\n",
    "weibullSet.sort()\n",
    "\n",
    "#print(\"Uniform set: %s\" % uniformSet)\n",
    "#print(\"Normal set:  %s\" % gaussianSet)\n",
    "#print(\"Pareto set:  %s\" % paretoSet)\n",
    "#print(\"Weibull set: %s\" % weibullSet)\n",
    "\n",
    "uniformHistogram = Counter(round(x,0) for x in uniformSet)\n",
    "normalHistogram = Counter(round(x,0) for x in gaussianSet)\n",
    "paretoHistogram = Counter(round(x,0) for x in paretoSet)\n",
    "weibullHistogram = Counter(round(x,0) for x in weibullSet)\n",
    "\n",
    "plt.bar([x for x in uniformHistogram.keys()], uniformHistogram.values())\n",
    "plt.title(\"Uniform Distribution PDF\")\n",
    "plt.show()\n",
    "\n",
    "plt.bar([x for x in normalHistogram.keys()], normalHistogram.values())\n",
    "plt.title(\"Normal/Gaussian Distribution PDF\")\n",
    "plt.show()\n",
    "\n",
    "plt.bar([x for x in paretoHistogram.keys()], paretoHistogram.values())\n",
    "plt.title(\"Pareto (80/20 rule) Distribution PDF\")\n",
    "plt.show()\n",
    "\n",
    "plt.bar([x for x in weibullHistogram.keys()], weibullHistogram.values())\n",
    "plt.title(\"Weibul Distribution PDF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08423f2-6c5c-4b77-afc2-2731c991b6a3",
   "metadata": {},
   "source": [
    "### Discrete Probability Distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4876787-5057-4887-87ae-d58c80740f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter \n",
    "\n",
    "alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g']\n",
    "corpus = []\n",
    "for i in range(1,100):\n",
    "    word = \"\"\n",
    "    for j in range (1,3):\n",
    "        word = word + alphabet[random.randint(0,6)]\n",
    "    corpus.append(word)\n",
    "corpus.sort()\n",
    "print(\"Corpus: %s\" % corpus)\n",
    "\n",
    "corpusHistogram = Counter(x for x in corpus)\n",
    "print(\"\\n Historgram Data: %s\" % corpusHistogram)\n",
    "plt.bar([x for x in corpusHistogram.keys()], corpusHistogram.values())\n",
    "plt.gcf().set_size_inches(20, 5)\n",
    "plt.title(\"2-gram/digraph Distribution PMF\")\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ebf2f-357f-4162-8fdd-bea1dd0e0ca0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Bayes Formula (or the Third Most Important Formula in Machine Learning)\n",
    "Devised by Thomas Bayes in the 1700's, but now a critical for predictive modeling and analysis:\n",
    "\n",
    "![Bayes Formula](https://wikimedia.org/api/rest_v1/media/math/render/svg/c1a7279a1639d92d751e0f2d3aa54e62a2ddb1e8)\n",
    "\n",
    "The common formulation and use deals with seemingly likely outcomes weighted by other information:\n",
    "\n",
    "![BayesUse](https://wikimedia.org/api/rest_v1/media/math/render/svg/b01f679001d8f19c6c6036f1ac66ca3c3f400258)\n",
    "\n",
    "### The Monte Hall Problem\n",
    "Imagine you are at a game show, with three doors. One has a good prize, two have a bad prize (goat?). You choose one:\n",
    "\n",
    "![Monte Hall Step 1](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Monty_open_door.svg/220px-Monty_open_door.svg.png)\n",
    "\n",
    "Then the host reveals one door with two assumptions:\n",
    "\n",
    "1) The Host will NOT reveal the door you chose first\n",
    "2) The Host will NOT reveal the prize\n",
    "\n",
    "The host then asks if you want to keep your original door choice, or switch to the remaining door. This is your final choice and you will get the prize (or goat) behind your final decision. What choice should you make to maximize your probability of winning the prize?\n",
    "\n",
    "![Monte Hall Step 2](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Monty_Hall_Problem_-_Standard_probabilities.svg/330px-Monty_Hall_Problem_-_Standard_probabilities.svg.png)\n",
    "\n",
    "Let's prove the Monte Hall solution using Monte Carlo (no relation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d11853-b944-44ab-96f2-7aa1c14b62f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import math, enum, random\n",
    "\n",
    "\n",
    "# Let D = number of doors in the game\n",
    "D=3\n",
    "# Let us run these many trials (large number for sufficient statistics)\n",
    "TRIALS=100\n",
    "# These will be our two strategies\n",
    "STAY_STRATEGY = 0\n",
    "SWITCH_STRATEGY = 1\n",
    "\n",
    "#\n",
    "# We want to use a strategy pattern for the game, so lets make some strategy behavior classes...\n",
    "#\n",
    "# Based Monte Hall decider always stays\n",
    "class BaseMonteHallDecider:\n",
    "    def decide(self, x):\n",
    "        return STAY_STRATEGY\n",
    "\n",
    "class SavvyMonteHallDecider(BaseMonteHallDecider):\n",
    "    # Override the based class and this time always switch\n",
    "    def decide(self, x):\n",
    "        return SWITCH_STRATEGY\n",
    "\n",
    "class RandomMonteHallDecider(BaseMonteHallDecider):\n",
    "    # Override the base class and randomly choose STAY_STRATEGY(0) or SWITCH_STRATEGY(1)\n",
    "    def decide(self, x):\n",
    "        return random.randint(0,1)\n",
    "\n",
    "stayPlayer       = BaseMonteHallDecider()\n",
    "switchPlayer     = SavvyMonteHallDecider()\n",
    "indecisivePlayer = RandomMonteHallDecider()\n",
    "\n",
    "# Define a single instance/execution of the game\n",
    "# Param 1 = player strategy class\n",
    "# Param 2 = Number of doors in the game\n",
    "def monteHallGame(strategyBehavior, D):\n",
    "    # Create D Doors\n",
    "    doors = [0 for x in range(D)]\n",
    "    choice = [0 for x in range(D)]\n",
    "    \n",
    "    # Place the Prize\n",
    "    door_with_prize = random.randint(0, D-1)\n",
    "    doors[door_with_prize] = 1\n",
    "    \n",
    "    # Choose first door\n",
    "    door_I_choose  = random.randint(0, D-1)\n",
    "    choice[door_I_choose] = 1\n",
    "    second_choice_door = 0\n",
    "    # Remove all but two doors keeping with the rules of the game\n",
    "    if door_I_choose == door_with_prize:\n",
    "        second_choice_door = (door_I_choose + random.randint(1, D-1)) % D # If my door has the prize, leave a random other door closed\n",
    "        doors[second_choice_door] = 1\n",
    "    else:\n",
    "        second_choice_door = door_with_prize\n",
    "        doors[door_I_choose] = 1\n",
    "    \n",
    "    final_choice_door = door_I_choose\n",
    "   \n",
    "    action = strategyBehavior.decide(choice + doors)   # <--- Here is where the magic happens\n",
    "    if action >= 0.5: \n",
    "        # If switching, remove my door from choices my final choice is the remaining door\n",
    "        final_choice_door = second_choice_door\n",
    "    else:\n",
    "        final_choice_door = door_I_choose\n",
    "                \n",
    "    # return True (1) if we chose the prize, plus the initial door choice and second round options (for learning later)\n",
    "    return (door_with_prize == final_choice_door), choice, doors, action\n",
    "\n",
    "\n",
    "# Manage multiple independant expirements of the game\n",
    "def monteCarlo(strategyBehavior, D):\n",
    "    winCount = 0\n",
    "    for i in tqdm(range(TRIALS),unit=\"iter\"):\n",
    "        result, _, _, _ = monteHallGame(strategyBehavior, D) \n",
    "        winCount += result\n",
    "    return winCount\n",
    "\n",
    "stay_wins   = monteCarlo(stayPlayer, D)\n",
    "switch_wins = monteCarlo(switchPlayer, D)\n",
    "random_wins   = monteCarlo(indecisivePlayer, D)\n",
    "print('Results with %s doors and %s trials' % (D,TRIALS))\n",
    "print('Proportion of wins without switching: {:.2f}%'.format(100.0*stay_wins/TRIALS))\n",
    "print('Proportion of wins with switching:    {:.2f}%'.format(100.0*switch_wins/TRIALS))\n",
    "print('Proportion of wins with random:       {:.2f}%'.format(100.0*random_wins/TRIALS))\n",
    "labels = [\"Stay\", \"Switch\", \"Random\"]\n",
    "results = [stay_wins, switch_wins, random_wins]\n",
    "plt.bar(range(3), results)\n",
    "plt.title(\"Monte Hall Strategy Comparison\")\n",
    "plt.ylabel(\"P(win)\")\n",
    "plt.xticks(range(3),labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42887d9-0a14-42e3-a9ef-dc31579f1ff1",
   "metadata": {},
   "source": [
    "... but if you still don't believe it, here are some different perspectives:\n",
    "\n",
    "* From the movie '21': ['21' Movie depiction of Monte Hall](https://youtu.be/iBdjqtR2iK4)\n",
    "* My favorite [explination by Numberphile](https://youtu.be/4Lb-6rxZxx0)\n",
    "* A less mathmatical by very historical view [as explained by Vox](https://youtu.be/ggDQXlinbME)\n",
    "* An entertaining [explination by VSauce](https://youtu.be/TVq2ivVpZgQ) ... this guy is weird.\n",
    "\n",
    "\n",
    "\n",
    "So now that we have wrinkled our brains with Bayesian probability puzzles, let us put all this together and actually do a Machine Learning approach to creating an AI/ML-driven Monte Hall player behavior. Since we are doing ML, let's make a quick reinforcement learning agent to learn the best strategy for the Monte Hall problem. Here we will start with a first attempt, then adjust the decision point based on success/failure. The interesting thing about reinforcement learning is you don't need prior data - it can learn on the fly (\"on line learning\"), although typically you want to do pre-training in a simulated (even Monte Carlo) environment to bootstrap the model before you feild it! \n",
    "\n",
    "First, let's override an AI-based decider that overrides the earlier class but this time can learn a strategy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89dce0d-fdfb-4390-ba6b-cab39ed1020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "class AIMonteHallDecider(BaseMonteHallDecider):\n",
    "    # Member Variable\n",
    "    learner = Perceptron(alpha=1e-5,\n",
    "                max_iter = 1,\n",
    "                warm_start=1,\n",
    "                verbose=0)\n",
    "    \n",
    "    # New function for this class - batch training\n",
    "    def train(self, x, y):\n",
    "        self.learner.fit(x, y)\n",
    "\n",
    "    # Another new function - this one does a one-sample update\n",
    "    # long story, but fit() needs both classes represented, so lets make the opposite action/result\n",
    "    # and then severely de-weight the second example\n",
    "    def update(self, x, y):\n",
    "        counterexample = [x[i] for i in x] # Make a deep copy of the state/action space\n",
    "        counterexample[-1] = abs(x[-1] - 1) # ... but swap the action\n",
    "        self.learner.fit([x, counterexample], [y, abs(y-1)], sample_weight=[1.0, 0.000001])\n",
    "        \n",
    "    # Override the base class\n",
    "    def decide(self, x):\n",
    "        # We need to give the predictor a data SET, so we will give it both action options for the given state\n",
    "        results = self.learner.predict([x + [STAY_STRATEGY], x + [SWITCH_STRATEGY]])\n",
    "        estimated_prize_if_stay   = results[0] # Then parse out the STAY action estimate\n",
    "        estimated_prize_if_switch = results[1] # and the SWITCH action estimate\n",
    "        if estimated_prize_if_stay > estimated_prize_if_switch:\n",
    "            return STAY_STRATEGY\n",
    "        return SWITCH_STRATEGY\n",
    "\n",
    "class NNLearningMonteHallDecider(AIMonteHallDecider):\n",
    "    # Member Variable\n",
    "    learner = MLPRegressor(solver='sgd',       # solver{‘lbfgs’, ‘sgd’, ‘adam’}, default=’adam’\n",
    "            activation='relu',          # activation{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’\n",
    "            learning_rate=\"constant\",   # learning_rate{‘constant’, ‘invscaling’, ‘adaptive’}, default=’constant’\n",
    "            alpha=1e-5,\n",
    "            hidden_layer_sizes=(5),  # <-- Set this to 1 (less NN, more Perceptron-like), and it works fine\n",
    "            random_state=1,\n",
    "            max_iter=1,\n",
    "            warm_start=True,\n",
    "            shuffle=True,\n",
    "            momentum=0.9,\n",
    "            verbose=False\n",
    "            )\n",
    "\n",
    "AIPlayer = AIMonteHallDecider()\n",
    "NNPlayer = NNLearningMonteHallDecider()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f483713",
   "metadata": {},
   "source": [
    "### Using Vectors to Define State and Action Space\n",
    "\n",
    "We will base our approach on the [Q-Learning](https://en.wikipedia.org/wiki/Q-learning) core equation which looks like this:\n",
    "\n",
    "![Bellman Equation](https://wikimedia.org/api/rest_v1/media/math/render/svg/678cb558a9d59c33ef4810c9618baf34a9577686)\n",
    "\n",
    "Here, \"states\" are 'world conditions', such as which doors are available for selection, which doors we have selected so far, etc. \"actions\" are what 'moves' we can make in the game. For Monte Hall, there are only two moves: stay or switch door selection (discounting the initial random selection of the first door for simplitiy right now, plus we will account for that in the state space... for simplicity). Technically speaking this equation is used for the Q-Learning algorithm, which has nothing really to do with Perceptrons, but we are going to take the principles of the state/action space and use a classical model. If you are really interested in Reinforcement Learning, take a look at TD-Gammon, Neuro-Reinforcement Learning, and Q-Learning. Some libraries like DeepLearning4J, Torch, and others have Deep Reinforcement Learning based on this drastically overly-simplified perceptron concept here.\n",
    "\n",
    "\"Q\" is then the learning structure where we learn the probable 'reward' for taking an action given a certain state... so (S,a) stores (and we will learn) the state and action combinations.\n",
    "\n",
    "For completeness, let us assume we don't know if the initial door selection or the availability options of the remaining doors effects the game, so we will define the state space as follows:\n",
    "* each state element will be a zero or one (0,1)\n",
    "* The first **D** (**D** := number of doors) state element slots will define which door we selected first... so of the first **D** elements, only one will have the value of '1' (indicating out initial selection)\n",
    "* The next **D** state element slots will define the available doors on the second phase... so there will be exactly two slots with a value of '1' - the door we initially selected and on other random door that we could switch to\n",
    "* After the 2x**D** state element slots, the last slot will be the action we took (0 = stay, 1 = switch)\n",
    "\n",
    "We can then use an ML model to predict our 'reward' (or probability of winning) by asking the estimate of Q(S, stay) vs Q(S, switch) and go with the highest estimated return. In ML speak, we want the argmax(action) := Q(S, action) where **S** is our current state space and ***action*** is the proposed decision/action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this... it just cleans up something annoying...\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e67bf-0bbb-47b4-ba4d-3bbd3c8ea1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "latestSuccess = {\"AI\": 0, \"SWITCH\": 0, \"STAY\": 0, \"RAND\": 0, \"NN\": 0}\n",
    "successTracking = {\"AI\": [], \"SWITCH\": [], \"STAY\": [], \"RAND\": [], \"NN\": []}\n",
    "\n",
    "D=3\n",
    "TRIALS=5000\n",
    "WINDOW_FRAC=TRIALS/100\n",
    "\n",
    "# Input Schema: Q(State, action) \n",
    "# - First D slots designate the door player initially selected (so only one of the first three slots should be non-zero)\n",
    "# - Next D slots designate the doors remaining after a reveal (so only two of the last 3 slots should be non-zero)\n",
    "# - Last slot is the action (0 = stay, 1 = switch)\n",
    "# for y value, 1 = win, 0 = loss\n",
    "\n",
    "# We will first setup our perceptron with a 2-record initial 'training', and bias it against the recommended strategy (just for fun!)\n",
    "firstDoorChoice = [0 for _ in range(D)]\n",
    "finalOptionDoors = [0 for _ in range(D)]\n",
    "firstDoorChoice[0] = 1  # Say we chose the first door\n",
    "finalOptionDoors[0] = 1 # so first door is unopened at phase 2\n",
    "finalOptionDoors[1] = 1 # and let's make the second door also still closed/available\n",
    "action = STAY_STRATEGY  # Let's say we stayed with our original door\n",
    "result = 1              # and let's say we won\n",
    "\n",
    "QSA1 = firstDoorChoice + finalOptionDoors + [STAY_STRATEGY]\n",
    "QSA2 = firstDoorChoice + finalOptionDoors + [SWITCH_STRATEGY]\n",
    "#print(\"Q(S,a) := %s\" % QSA)\n",
    "AIPlayer.train([QSA1, QSA2], [1,0])  # Say we won by staying and lost by switching... just for fun \n",
    "NNPlayer.train([QSA1, QSA2], [1,0])  # Say we won by staying and lost by switching... just for fun \n",
    "\n",
    "# Now we can let our agent play the game a LOT of times and see how it improves over time...\n",
    "#for i in tqdm(range(5000),unit=\"games\",desc=\"Monte Hall RLAgent\"):\n",
    "for i in tqdm(range(TRIALS), unit=\"iter\"):\n",
    "    #Play the game one round with our AI Agent\n",
    "    result, firstDoorChoice, finalOptionDoors, action = monteHallGame(AIPlayer, D) \n",
    "    AIPlayer.update(firstDoorChoice + finalOptionDoors + [action], result) \n",
    "    latestSuccess[\"AI\"] = latestSuccess[\"AI\"] * (1 - 1/(TRIALS/WINDOW_FRAC)) + result / (TRIALS/WINDOW_FRAC)\n",
    "    successTracking[\"AI\"].append(latestSuccess[\"AI\"])\n",
    "    \n",
    "    # And a more complex multi-layer perceptron - i.e., and Neural Network\n",
    "    result, firstDoorChoice, finalOptionDoors, action = monteHallGame(NNPlayer, D) \n",
    "    AIPlayer.update(firstDoorChoice + finalOptionDoors + [action], result) \n",
    "    latestSuccess[\"NN\"] = latestSuccess[\"NN\"] * (1 - 1/(TRIALS/WINDOW_FRAC)) + result / (TRIALS/WINDOW_FRAC)\n",
    "    successTracking[\"NN\"].append(latestSuccess[\"NN\"])\n",
    "    \n",
    "    # And let the stay-player play too\n",
    "    result, _, _, _ = monteHallGame(stayPlayer, D) \n",
    "    latestSuccess[\"STAY\"] = latestSuccess[\"STAY\"] * (1 - 1/(TRIALS/WINDOW_FRAC)) + result / (TRIALS/WINDOW_FRAC)\n",
    "    successTracking[\"STAY\"].append(latestSuccess[\"STAY\"])\n",
    "    # And the smart switcher\n",
    "    result, _, _, _ = monteHallGame(switchPlayer, D) \n",
    "    latestSuccess[\"SWITCH\"] = latestSuccess[\"SWITCH\"] * (1 - 1/(TRIALS/WINDOW_FRAC)) + result / (TRIALS/WINDOW_FRAC)\n",
    "    successTracking[\"SWITCH\"].append(latestSuccess[\"SWITCH\"])\n",
    "    # And the \"its 50/50\", random picker\n",
    "    result, _, _, _ = monteHallGame(indecisivePlayer, D) \n",
    "    latestSuccess[\"RAND\"] = latestSuccess[\"RAND\"] * (1 - 1/(TRIALS/WINDOW_FRAC)) + result / (TRIALS/WINDOW_FRAC)\n",
    "    successTracking[\"RAND\"].append(latestSuccess[\"RAND\"])\n",
    "    \n",
    "# Let's graphically look at the values over time as it 'learned' \n",
    "idx = [ i for i in range(len(successRateTrace))]\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 5)\n",
    "#ax.plot(idx, thresholdTrace, color='0.5', label='threshold for decision')\n",
    "ax.plot(idx, successTracking[\"AI\"], color='#0000FF', label='Perceptron Learner')\n",
    "ax.plot(idx, successTracking[\"STAY\"], color='#FF0000', label='Stay-action')\n",
    "ax.plot(idx, successTracking[\"SWITCH\"], color='#00FF00', label='Switch-action')\n",
    "ax.plot(idx, successTracking[\"RAND\"], color='#AAAA00', label='random-action')\n",
    "ax.plot(idx, successTracking[\"NN\"], color='#990099', label='Neural Network')\n",
    "\n",
    "plt.xlabel(\"round\")\n",
    "plt.ylabel(\"rate\")\n",
    "plt.title(\"Reinforcement Learning Record\")\n",
    "plt.legend()\n",
    "plt.grid(True, which='both')\n",
    "plt.ylim(0,1)\n",
    "plt.gcf().set_size_inches(15,10)\n",
    "plt.show()\n",
    "\n",
    "print(\"Perceptron intercept: %s\" % AIPlayer.learner.intercept_)\n",
    "print(\"Perceptron input weights: %s\" % AIPlayer.learner.coef_)\n",
    "print(\"Perceptron intercept: %s\" % AIPlayer.learner.classes_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63a93f",
   "metadata": {},
   "source": [
    "Crazy, right? Shouldn't a more sophisticated Neural Network do at least as good as a Perceptron? What is going on? Adjust the door count (D), iteration count(TRIALS), and the number of layers in the Neural Network (line 39 with the 'hidden_layer_sizes' parameter) and see how it behaves. This brings up three **very important** rules of thumb when working with machine learning:\n",
    "\n",
    "1. **Always** run your metrics and have target Measures of Performance (MoP)\n",
    "1. **Always** have a baseline for comparison (even if it is a \"random chooser\" or null-hypothesis)\n",
    "1. **Always** keep [Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_razor) in mind and don't over complicate your model space\n",
    "\n",
    "A correllary to the second point is that if your classifier (for a two class problem) is correct less than 50% of the time, reverse its output.\n",
    "\n",
    "Another way to state point three is K.I.S.S. (Keep it simple _____). A more robust and rigorous treatment of this principle and how to analyze it can be found in [VC-Dimensional analysis](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension) and [PAC Learning](https://en.wikipedia.org/wiki/Probably_approximately_correct_learning) (which is perhaps my favorite term in all of ML material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7ff15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
