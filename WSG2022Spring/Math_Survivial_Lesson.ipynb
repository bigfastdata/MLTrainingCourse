{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Crash Course on Surviving the Math\nWeek of July 25, 2022\n\n## Chapter 2 - First, a Minimal Intro to Python\nFor a deeper dive read Chapter 2 and as a quick reference, see [the Python 3 Cheat Sheet](https://www.pythoncheatsheet.org/cheatsheet/basics).","metadata":{"tags":[]},"id":"d1d189ab-5e1e-4c8d-b9d3-1f97d062ffae"},{"cell_type":"code","source":"# This is a Python Commentm which starts with a '#' sign\n# Below is Python Code\n\n# This is how you import additional functionality via libraries:\nimport math\n\n# This is how you declare and assign a scalar variable. Variables contain values that can also change.\nmyValue = 234.234\n\n# This is how you declare and assign an array variable of multiple values (Numpy will use proper 'vectors')\nmyVector = [1, 1, 2, 3, 5, 8, 13]\n\n# This is how you print out the value of a variable\nprint(\"My scalar value is %s\" % myValue)\nprint(\"My vector has the list of values: %s\" % myVector)\nprint(\"My vector's first element (zero-th index): %s\" % myVector[0])\n\n# You can do math:\nprint(\"Value * 2 = %s\" % (myValue * 2))\nprint(\"Value / 2 = %s\" % (myValue / 2))\nprint(\"Square root of Value = %s\" % (math.sqrt(myValue)))\nprint(\"Square root of Value = %s\" % (myValue ** 0.5))\n\n# You can manipulate the array:\nmyVector.reverse()\nprint(\"Reversed vector has the list of values: %s\" % myVector)\nprint(\"Reversed vector's first element (zero-th index): %s\" % myVector[0])","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"My scalar value is 234.234\nMy vector has the list of values: [1, 1, 2, 3, 5, 8, 13]\nMy vector's first element (zero-th index): 1\nValue * 2 = 468.468\nValue / 2 = 117.117\nSquare root of Value = 15.304705158871895\nSquare root of Value = 15.304705158871895\nReversed vector has the list of values: [13, 8, 5, 3, 2, 1, 1]\nReversed vector's first element (zero-th index): 13\n","output_type":"stream"}],"id":"6b01370e-5035-4309-9599-5793dbb2072d"},{"cell_type":"markdown","source":"## Chapter 3 - Visualizations\n\n","metadata":{},"id":"1b760a0e-8e84-4c6b-892a-d6c0f7f56833"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"4f67758a-9d38-4f06-968b-2f9027db6c37"},{"cell_type":"markdown","source":"## Chapter 4 - Linear Algebra\n### The Second Most Imporant Equation in Machine Learning\nThere are [may different 'distance' approaches](https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa), but the most common is one we are familiar with by other names and simpler forms:\n![Euclidian Distance](https://miro.medium.com/max/844/0*wv6oFAVd0_PQ50mX)\n\n\nIn code:","metadata":{},"id":"a513f6a0-d39d-4bc9-9aab-eeef276c9a97"},{"cell_type":"code","source":"def euclidianDistance(a, b):\n    result = 0\n    if len(a) == len(b):\n        summation = 0\n        for i in range(len(a)):\n            summation = (a[i] - b[i]) ** 2\n        result = summation ** 0.5\n    else:\n        print(\"WARNING: array sizes must be the same!\")\n    return result","metadata":{"trusted":true},"execution_count":11,"outputs":[],"id":"2169f78a-2613-413c-b82f-ff5f0646b5b9"},{"cell_type":"markdown","source":"... but in the end, these are all forms of the [Minkowski Distance](https://en.wikipedia.org/wiki/Minkowski_distance): \n\n![Minkowski Distance](https://miro.medium.com/max/902/0*UbbyH2MUPb5ZBa64)\n\nIn code form:","metadata":{},"id":"98b92b08-5092-4073-a06b-f91ed000021f"},{"cell_type":"code","source":"def minkowski(a, b, p):\n    result = 0\n    if len(a) == len(b):\n        summation = 0\n        maxDiff = 0\n        for i in range(len(a)):\n            if p < 500:\n                summation += abs(a[i] - b[i]) ** p\n            maxDiff = max(maxDiff, abs(a[i] - b[i]))\n        if p > 500:\n            result = maxDiff\n        else:\n            result = summation ** (1.0/p)\n    else:\n        print(\"WARNING: array sizes must be the same!\")\n    return result","metadata":{"trusted":true},"execution_count":36,"outputs":[],"id":"39d45682-2263-4833-9583-407f28ce5bf3"},{"cell_type":"markdown","source":"\nThe value of 'p' transforms the equation from Manhattan distance (p=0, a.k.a. L1-Norm) to Euclidian distance (p=1, a.k.a. L2-Norm), or [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance) as p approaches infinitiy. For fractional values of 'p' where 0 < p < 1, the 'Agrawal' distance is extremely useful to mitigate the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). \n\n![Variations of P](https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/2D_unit_balls.svg/967px-2D_unit_balls.svg.png)\n\n### In Python","metadata":{},"id":"a7a99423-e402-4f6f-a51c-9f8f66863728"},{"cell_type":"code","source":"import random\n\nN = 4\nvec0 = [ 0 for x in range(N)]\nvec1 = [random.gauss(5, 2) for x in range(N)] \nvec2 = [random.gauss(10, 2) for x in range(N)] \nvec1.sort()\nvec2.sort()\n\nprint(\"Vector 0: %s\" % vec0)\nprint(\"Vector 1: %s\" % vec1)\nprint(\"Vector 2: %s\" % vec2)\n\nprint(\"L1-Norm of Vector 1:{:>13,.2f}\".format(minkowski(vec1, vec0, 1)))\nprint(\"L1-Norm of Vector 2:{:>13,.2f}\".format(minkowski(vec2, vec0, 1)))\nprint(\"L2-Norm of Vector 1:{:>13,.2f}\".format(minkowski(vec1, vec0, 2)))\nprint(\"L2-Norm of Vector 2:{:>13,.2f}\".format(minkowski(vec2, vec0, 2)))\n\nprint(\"Agrawal   Distance: {:>13,.2f}\".format(minkowski(vec1, vec2,    0.1)))\nprint(\"Manhattan Distance: {:>13,.2f}\".format(minkowski(vec1, vec2,    1)))\nprint(\"Euclidian Distance: {:>13,.2f}\".format(minkowski(vec1, vec2,    2)))\nprint(\"Chebyshev Distance: {:>13,.2f}\".format(minkowski(vec1, vec2, 3000)))","metadata":{"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Vector 0: [0, 0, 0, 0]\nVector 1: [2.9486791473765464, 4.090180962461589, 4.877534016012555, 5.09830317190807]\nVector 2: [8.738949029707832, 8.774131075313127, 9.94466268655835, 10.912741353699397]\nL1-Norm of Vector 1:        17.01\nL1-Norm of Vector 2:        38.37\nL2-Norm of Vector 1:         8.67\nL2-Norm of Vector 2:        19.27\nAgrawal   Distance:  5,577,346.99\nManhattan Distance:         21.36\nEuclidian Distance:         10.72\nChebyshev Distance:          5.81\n","output_type":"stream"}],"id":"850ee313-e74c-4434-88ce-e3361e40240c"},{"cell_type":"markdown","source":"## Chapter 5 - Statistics\n\n### Continuous Probability Distributions\n\n","metadata":{},"id":"fefa7df3-5e2a-423c-a25d-608aa5f3204e"},{"cell_type":"code","source":"# Uniform random between 0.0-1.0\nrandX = random.random()\n# random.gauss(mu, sigma)\n# random.uniform(a, b)\n# random.paretovariate(alpha)\n# random.weibullvariate(alpha, beta)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"7b1f3141-fc80-460d-b42d-5ffc973a8a1d"},{"cell_type":"markdown","source":"### Discrete Probability Distributions\n\n","metadata":{},"id":"d08423f2-6c5c-4b77-afc2-2731c991b6a3"},{"cell_type":"code","source":"alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g']\ncorpus = []\nfor i in range(1,100):\n    word = \"\"\n    for j in range (1,3):\n        word = word + alphabet[random.randint(0,6)]\n    corpus.append(word)\n\nprint(\"Corpus: %s\" % corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"e4876787-5057-4887-87ae-d58c80740f56"},{"cell_type":"markdown","source":"\n### Statistics and Randomness in Python\n\n","metadata":{},"id":"3375b9e0-9f20-4af9-bfd6-9ebb2fad3b60"},{"cell_type":"code","source":"\n","metadata":{},"execution_count":null,"outputs":[],"id":"6139c465-752c-4bd6-8b32-b05e97019b6e"},{"cell_type":"markdown","source":"## Chapter 6 - Probability\n\n### Joint Probability\n\n### Priors\n\n### Bayes Formula (or the Third Most Important Formula in Machine Learning)\nDevised by Thomas Bayes in the 1700's, but now a critical for predictive modeling and analysis:\n\n![Bayes Formula](https://wikimedia.org/api/rest_v1/media/math/render/svg/c1a7279a1639d92d751e0f2d3aa54e62a2ddb1e8)\n\nThe common formulation and use deals with seemingly likely outcomes weighted by other information:\n\n![BayesUse](https://wikimedia.org/api/rest_v1/media/math/render/svg/b01f679001d8f19c6c6036f1ac66ca3c3f400258)\n\n### The Monte Hall Problem\nImagine you are at a game show, with three doors. One has a good prize, two have a bad prize (goat?). You choose one:\n\n![Monte Hall Step 1](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Monty_open_door.svg/220px-Monty_open_door.svg.png)\n\nThen the host reveals one door with two assumptions:\n\n1) The Host will NOT reveal the door you chose first\n2) The Host will NOT reveal the prize\n\nThe host then asks if you want to keep your original door choice, or switch to the remaining door. This is your final choice and you will get the prize (or goat) behind your final decision. What choice should you make to maximize your probability of winning the prize?\n\n![Monte Hall Step 2](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Monty_Hall_Problem_-_Standard_probabilities.svg/330px-Monty_Hall_Problem_-_Standard_probabilities.svg.png)\n\nLet's prove the Monte Hall solution using Monte Carlo (no relation):","metadata":{},"id":"2f5ebf2f-357f-4162-8fdd-bea1dd0e0ca0"},{"cell_type":"code","source":"import random\n\n# Let D = number of doors in the game\nD=3\n\n# Let us run these many trials (large number for sufficient statistics)\nTRIALS=100\n\n# These will be our two strategies\nSWITCH_STRATEGY=True\nSTAY_STRATEGY=False\n\n# Define a single instance/execution of the game\ndef monteHallGame(strategy):\n    # Create D Doors\n    door_list = [x+1 for x in range(D)]\n    # Place the Prize\n    door_with_prize = random.randint(1, D)\n    # Choose first door\n    door_I_choose  = random.randint(1, D)\n    # Remove all but two doors keeping with the rules of the game\n    remaining_doors_list = [door_I_choose]\n    if door_I_choose == door_with_prize:\n        remaining_doors_list.append(random.randint(1, D))\n    else:\n        remaining_doors_list.append(door_with_prize)\n        \n    if strategy == SWITCH_STRATEGY: \n        # If switching, remove my door from choices\n        remaining_doors_list.remove(door_I_choose)\n        # Then my final choice is the remaining door\n        door_I_choose = remaining_doors_list[0] \n                \n    # return True (1) if we chose the prize\n    return door_with_prize == door_I_choose\n\n# Manage multiple independant expirements of the game\ndef monteCarlo(strategy):\n    winCount = 0\n    for i in range(TRIALS):\n        winCount += monteHallGame(strategy) \n    return winCount\n\n\nswitch_wins = monteCarlo(SWITCH_STRATEGY)\nstay_wins   = monteCarlo(STAY_STRATEGY)\n\nprint('Results with %s doors and %s trials' % (D,TRIALS))\nprint('Proportion of wins without switching: {:.2f}%'.format(100.0*stay_wins/TRIALS))\nprint('Proportion of wins with switching: {:.2f}%'.format(100.0*switch_wins/TRIALS))","metadata":{"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Results with 3 doors and 100 trials\nProportion of wins without switching: 34.00%\nProportion of wins with switching: 70.00%\n","output_type":"stream"}],"id":"61d11853-b944-44ab-96f2-7aa1c14b62f2"},{"cell_type":"markdown","source":"... but if you still don't believe it, here are some different perspectives:\n\n* From the movie '21': ['21' Movie depiction of Monte Hall](https://youtu.be/iBdjqtR2iK4)\n* My favorite [explination by Numberphile](https://youtu.be/4Lb-6rxZxx0)\n* A less mathmatical by very historical view [as explained by Vox](https://youtu.be/ggDQXlinbME)\n* An entertaining [explination by VSauce](https://youtu.be/TVq2ivVpZgQ) ... this guy is weird.","metadata":{},"id":"d42887d9-0a14-42e3-a9ef-dc31579f1ff1"}]}