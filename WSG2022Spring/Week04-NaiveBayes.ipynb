{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37ebf0a",
   "metadata": {},
   "source": [
    "# Week 4 - Naive Bayes Inferencing and Classification\n",
    "Week Four - August 15, 2022\n",
    "\n",
    "\n",
    "Reminder, the author has all of the code available on [Github](https://github.com/joelgrus/data-science-from-scratch)\n",
    "\n",
    "The data set is included in this repo, but sourced from [Kaggle](https://www.kaggle.com/datasets/bagavathypriya/spam-ham-dataset)\n",
    "\n",
    "[A really good intro to Naive Bayes and Bayesian modeling from Veritasium](https://youtu.be/R13BD8qKeTg)\n",
    "\n",
    "## Chapter 13 - Naive Bayes Modeling\n",
    "\n",
    "### Review of Bayes Theorem and Creating a Decision Space\n",
    "Recall the Bayesian formula: <code>P(Y|X) = P(X|Y) P(Y) / P(X)</code>\n",
    "\n",
    "For a spam filter <code>Y = {spam, not_spam} </code>, and <code>X = {word_1, word_2, ... word_k} </code>\n",
    "\n",
    "So, <code>P(spam | X) = P(X | spam) * P(spam) / P(X) </code>\n",
    "Likewise, <code>P(not_spam | X) = P(X | not_spam) * P(not_spam) / P(X) </code>\n",
    "\n",
    "So to predict if the email is spam or not, we want to compare <code>P(spam | X)</code> with <code>P(not_spam | X)</code> and go with the higher probability:\n",
    "\n",
    " <code> P(X | spam) * P(spam) / P(X) (?) P(X | not_spam) * P(not_spam) / P(X)</code>\n",
    "\n",
    "Since both probabilities have the same denominator, we can eliminate it for numberical stability (and less computing).\n",
    "\n",
    "<code> P(X | spam) * P(spam)  (?)  P(X | not_spam) * P(not_spam) </code>\n",
    "\n",
    "### Bayesian Optimal Solutions - Theoretically the Best\n",
    "\n",
    "\"X\" is really the set of all features, or for spam determination, the set of all word counts in the email set, so this is really:\n",
    "\n",
    "<code> P(word_1, word_2, ..., word_k | spam) * P(spam)  (?)  P(word_1, word_2, ..., word_k  | not_spam) * P(not_spam) </code>\n",
    "\n",
    "By the chain rule of probability this expands out (just showing the P(spam...) side for simplicity):\n",
    "\n",
    "<code> P(spam) * P(word_1 | spam) *  P(word_1 | word_2, spam) * ... * (word_1 | word_2, ..., word_k, spam) * P(word_2 | spam) *  P(word_2 | word_1, spam) * ... * (word_2 | word_1, ..., word_k, spam) * ... </code>\n",
    "\n",
    "This full expansion will give us **the absolutly most accurate classifier possible** called the _Bayesian Optimimal Solution_. However, it is unlikely we can really charactorize all the joint and conditional probabilities, and even if that was available in the data, it would be computationally intractable!  Therefore a _Bayesian Optimimal Solution_ is usually just considered as the theoretically ideal classifier and we need to use heuristic methods to approximate this optimal solution but with an algorithm that is tractable. \n",
    "\n",
    "### Assuming Independance between Features is Naive\n",
    "\n",
    "We can leverage the usually inaccurate _independant and identically distributed_ (i.i.d.) assumption among the features. By _independant_ we will assume that for any two features _a_ and _b_ there is zero correlation between the two. This is a fairly naive assumption, but it turns out will serve well in this algorithm. By _identically distributed_ we will assume that each feature is equally weighted and no one feature is more important than any other in determining the label.\n",
    "\n",
    "Despite these assumptions being naive, they still provide a remarkably accurate classifier.\n",
    "\n",
    "### Numerical Stability\n",
    "\n",
    "#### Log-Likelihood\n",
    "Multiplying a lot of numbers between (0,1) tends to cause _underflow_ issues in computers, so we can take the  log of the probabilities to keep the numbers stable, and then take the exponent of the answer to conver back.\n",
    "\n",
    "#### LaPlace Smoothing\n",
    " What if one of the probabilities is zero? That is, if there are no examples in the training data for one word in one of the classes/labels, then the P(y|x) := 0/|y| * P(y). In order to avoid this, we can either seed every token occurance with a small starting value, or catch any zero-valued probabilities and substitute it with a rather small _alpha_ value (e.g., 1E-7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f89c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input from file data/spamdata.csv\n",
      "Reading input from file data/hamdata.csv\n",
      "Counter({(False, False): 1})\n",
      "\n",
      "\n",
      "Training set size:  1\n",
      "Total words/features:  27\n",
      "spammiest_words [\"08452810075over18's\", 'wkly', 'fa', 'comp', 'cup', 'tkts', 'win', 'apply', 'rate', 'free']\n",
      "hammiest_words ['std', 'text', \"c's\", 'txt', 'may', 'final', 't', '87121', 'question', '2005']\n"
     ]
    }
   ],
   "source": [
    "# Attribution: https://github.com/joelgrus/data-science-from-scratch/blob/master/scratch/naive_bayes.py\n",
    "\n",
    "from typing import Set\n",
    "import re\n",
    "\n",
    "def tokenize(text: str) -> Set[str]:\n",
    "    text = text.lower()                         # Convert to lowercase,\n",
    "    all_words = re.findall(\"[a-z0-9']+\", text)  # extract the words, and\n",
    "    return set(all_words)                       # remove duplicates.\n",
    "\n",
    "assert tokenize(\"Data Science is science\") == {\"data\", \"science\", \"is\"}\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool\n",
    "\n",
    "from typing import List, Tuple, Dict, Iterable\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k  # smoothing factor\n",
    "\n",
    "        self.tokens: Set[str] = set()\n",
    "        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.token_ham_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.spam_messages = self.ham_messages = 0\n",
    "\n",
    "    def train(self, messages: Iterable[Message]) -> None:\n",
    "        for message in messages:\n",
    "            # Increment message counts\n",
    "            if message.is_spam:\n",
    "                self.spam_messages += 1\n",
    "            else:\n",
    "                self.ham_messages += 1\n",
    "\n",
    "            # Increment word counts\n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] += 1\n",
    "\n",
    "    def _probabilities(self, token: str) -> Tuple[float, float]:\n",
    "        \"\"\"returns P(token | spam) and P(token | not spam)\"\"\"\n",
    "        spam = self.token_spam_counts[token]\n",
    "        ham = self.token_ham_counts[token]\n",
    "\n",
    "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
    "        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
    "\n",
    "        return p_token_spam, p_token_ham\n",
    "\n",
    "    def predict(self, text: str) -> float:\n",
    "        text_tokens = tokenize(text)\n",
    "        log_prob_if_spam = log_prob_if_ham = 0.0\n",
    "\n",
    "        # Iterate through each word in our vocabulary.\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "\n",
    "            # If *token* appears in the message,\n",
    "            # add the log probability of seeing it;\n",
    "            if token in text_tokens:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_ham += math.log(prob_if_ham)\n",
    "\n",
    "            # otherwise add the log probability of _not_ seeing it\n",
    "            # which is log(1 - probability of seeing it)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
    "\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
    "\n",
    "messages = [Message(\"spam rules\", is_spam=True),\n",
    "            Message(\"ham rules\", is_spam=False),\n",
    "            Message(\"hello ham\", is_spam=False)]\n",
    "\n",
    "model = NaiveBayesClassifier(k=0.5)\n",
    "model.train(messages)\n",
    "\n",
    "assert model.tokens == {\"spam\", \"ham\", \"rules\", \"hello\"}\n",
    "assert model.spam_messages == 1\n",
    "assert model.ham_messages == 2\n",
    "assert model.token_spam_counts == {\"spam\": 1, \"rules\": 1}\n",
    "assert model.token_ham_counts == {\"ham\": 2, \"rules\": 1, \"hello\": 1}\n",
    "\n",
    "text = \"hello spam\"\n",
    "\n",
    "probs_if_spam = [\n",
    "    (1 + 0.5) / (1 + 2 * 0.5),      # \"spam\"  (present)\n",
    "    1 - (0 + 0.5) / (1 + 2 * 0.5),  # \"ham\"   (not present)\n",
    "    1 - (1 + 0.5) / (1 + 2 * 0.5),  # \"rules\" (not present)\n",
    "    (0 + 0.5) / (1 + 2 * 0.5)       # \"hello\" (present)\n",
    "]\n",
    "\n",
    "probs_if_ham = [\n",
    "    (0 + 0.5) / (2 + 2 * 0.5),      # \"spam\"  (present)\n",
    "    1 - (2 + 0.5) / (2 + 2 * 0.5),  # \"ham\"   (not present)\n",
    "    1 - (1 + 0.5) / (2 + 2 * 0.5),  # \"rules\" (not present)\n",
    "    (1 + 0.5) / (2 + 2 * 0.5),      # \"hello\" (present)\n",
    "]\n",
    "\n",
    "p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam))\n",
    "p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham))\n",
    "\n",
    "# Should be about 0.83\n",
    "assert model.predict(text) == p_if_spam / (p_if_spam + p_if_ham)\n",
    "\n",
    "def drop_final_s(word):\n",
    "    return re.sub(\"s$\", \"\", word)\n",
    "\n",
    "\n",
    "import glob, re\n",
    "    \n",
    "# modify the path to wherever you've put the files\n",
    "path = 'data/*amdata.csv'\n",
    "  \n",
    "data: List[Message] = []\n",
    "    \n",
    "# glob.glob returns every filename that matches the wildcarded path\n",
    "for filename in glob.glob(path):\n",
    "    print(\"Reading input from file %s\" % filename)\n",
    "    is_spam = \"ham\" not in filename\n",
    "    \n",
    "    # There are some garbage characters in the emails, the errors='ignore'\n",
    "    # skips them instead of raising an exception.\n",
    "    with open(filename, errors='ignore') as email_file:\n",
    "        for line in email_file:\n",
    "                # The book used files per email. This data set is email per line\n",
    "                data.append(Message(line, is_spam))\n",
    "                break  # done with this file\n",
    "    \n",
    "import random\n",
    "from scratch.machine_learning import split_data\n",
    "    \n",
    "random.seed(0)      # just so you get the same answers as me\n",
    "train_messages, test_messages = split_data(data, 0.75)\n",
    "    \n",
    "model = NaiveBayesClassifier()\n",
    "model.train(train_messages)\n",
    "    \n",
    "from collections import Counter\n",
    "    \n",
    "predictions = [(message, model.predict(message.text))\n",
    "               for message in test_messages]\n",
    "    \n",
    "# Assume that spam_probability > 0.5 corresponds to spam prediction\n",
    "# and count the combinations of (actual is_spam, predicted is_spam)\n",
    "confusion_matrix = Counter((message.is_spam, spam_probability > 0.5)\n",
    "                           for message, spam_probability in predictions)\n",
    "    \n",
    "print(confusion_matrix)\n",
    "   \n",
    "def p_spam_given_token(token: str, model: NaiveBayesClassifier) -> float:\n",
    "    # We probably shouldn't call private methods, but it's for a good cause.\n",
    "    prob_if_spam, prob_if_ham = model._probabilities(token)\n",
    "    \n",
    "    return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
    "    \n",
    "words = sorted(model.tokens, key=lambda t: p_spam_given_token(t, model))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Training set size: \", len(train_messages) )\n",
    "print(\"Total words/features: \", len(model.tokens))\n",
    "print(\"spammiest_words\", words[-10:])\n",
    "print(\"hammiest_words\", words[:10])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f11754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301973c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
